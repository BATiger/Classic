{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mainf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1FypPzut4hmT5XM-rd6eK6MUcg4Tl45_-",
      "authorship_tag": "ABX9TyPEpr5UZegChiLQr04Ky4Jq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "343f3fdfe6f54390a75b1ec40c6d77f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8ddba6008295449cb43850dd5eb034ed",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a65941c9ec8d40359079b865c75819d6",
              "IPY_MODEL_f6a15c2c421e4c47a98cd86d3b69a6da"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "8ddba6008295449cb43850dd5eb034ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "a65941c9ec8d40359079b865c75819d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_876ff3bc08924349a5aa90e43bf971d0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f41722472fee46d599387d19f968838a"
          },
          "model_module_version": "1.5.0"
        },
        "f6a15c2c421e4c47a98cd86d3b69a6da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9daf595d34a844bdbb81aefd9ab5e157",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:03&lt;00:00, 167MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b702f8f65cb24abea18a99d3915457b0"
          },
          "model_module_version": "1.5.0"
        },
        "876ff3bc08924349a5aa90e43bf971d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "f41722472fee46d599387d19f968838a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "9daf595d34a844bdbb81aefd9ab5e157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "b702f8f65cb24abea18a99d3915457b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BATiger/Classic/blob/main/mainf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU6gAMtiw8D5",
        "outputId": "0eb2cb75-fce3-4415-ecac-f4e5d50d2070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeMkEqJ5nb5h"
      },
      "source": [
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/MVQA')\n",
        "\n",
        "import processDataNew as processData\n",
        "import VGG16\n",
        "from ques_emb2 import QuestionEmbedding \n",
        "from img_emb import ImageEmbedding\n",
        "#from san1layer import Attention\n",
        "from san import Attention\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  \n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "\n",
        "use_cuda = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElMHSnodre8f",
        "outputId": "fd38dc38-8348-4fa7-fb1a-ca9c2fab78cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "data = processData.getAllText('/content/drive/My Drive/Colab Notebooks/MVQA/alldata.json')\n",
        "testdata = processData.getAllText('/content/drive/My Drive/Colab Notebooks/MVQA/testset.json')\n",
        "#traindata = processData.getAllText('/content/drive/My Drive/Colab Notebooks/MVQA/trainset.json')\n",
        "traindata = processData.getAllText('/content/drive/My Drive/Colab Notebooks/MVQA/faulttrainset.json')\n",
        "\n",
        "\n",
        "questions = processData.getQuestions(data)\n",
        "answers = processData.getAnswers(data)\n",
        "imgs_name = processData.get_image_name(data)\n",
        "\n",
        "dicQuestion = processData.getDic(questions)\n",
        "ans_dic = list(set(answers))\n",
        "\n",
        "questions_train = processData.getQuestions(traindata)\n",
        "imgs_name_train = processData.get_image_name(traindata)\n",
        "answers_train = processData.getAnswers(traindata)\n",
        "\n",
        "questions_test = processData.getQuestions(testdata)\n",
        "imgs_name_test = processData.get_image_name(testdata)\n",
        "answers_test = processData.getAnswers(testdata)\n",
        "\n",
        "ques_one_hot_train = []\n",
        "for question in questions_train:\n",
        "    one_hot_vector = processData.getVecRep(dicQuestion,question.lower())\n",
        "    ques_one_hot_train.append(one_hot_vector)\n",
        "\n",
        "\n",
        "\n",
        "answer_to_vec_train = []\n",
        "for answer in answers_train:\n",
        "    if isinstance(answer,str):\n",
        "          answer = answer.lower()\n",
        "    vector = processData.getVecRep2(ans_dic,answer)\n",
        "    answer_to_vec_train.append(vector)\n",
        "\n",
        "#double the training set\n",
        "ques_one_hot_train.extend(ques_one_hot_train)\n",
        "answer_to_vec_train.extend(answer_to_vec_train)\n",
        "for i in range(len(imgs_name_train)):\n",
        "      imgs_name_train.append(imgs_name_train[i]+'c')\n",
        "\n",
        "\n",
        "ques_one_hot_test = []\n",
        "for question in questions_test:\n",
        "    one_hot_vector = processData.getVecRep(dicQuestion,question.lower())\n",
        "    ques_one_hot_test.append(one_hot_vector)\n",
        "\n",
        "answer_to_vec_test = []\n",
        "for answer in answers_test:\n",
        "    if isinstance(answer,str):\n",
        "        answer = answer.lower()\n",
        "    vector = processData.getVecRep2(ans_dic,answer)\n",
        "    answer_to_vec_test.append(vector)\n",
        "\n",
        "print(len(ans_dic))\n",
        "print(ans_dic)\n",
        "print(len(dicQuestion))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "517\n",
            "[2, 4, 'pineal gland', 12, 'non-contrast', 'enhancement of vessels', 'right kidney', 'posteriorly', 'mr - flair', 'irregular', 'hypoxic ischemic injury', 'diffusion weighted imaging (dwi)', 'sharp costophrenic angles', 'sulcal effacement', 'ivc', 'maybe', 'increased opacity in the left retrocardiac region', 't2', 'the surrounding tissue', 'left rectus abdominus', 'semi-upright position', 'ribs', 'sternotomy wires and surgical clips', 'enlarged', 'motion', 'metastases, infection/abcess, glioblastoma', 'abdomen', 'there is massive cerebral hemisphere edema', 'left lung', 'hydrocephalus', '5%', 'right upper lobe', 'abcess', 'sinusitis', 'concave', 'flair', 'ring-enhancing lesions', 'cecum', 'x-ray plain film', 'just one', 't2-weighted', 'pa', 'right pica', 'left temporal horn', 'not sure.', 'multilobulated', 'contrast in the intestines', 'cxr', 'plain film xray', 'bilateral frontal lobes', 'mr - t2 weighted', 'ring enhancing lesion', 't5', 'upper lobes', 'ct with gi and iv contrast', 'coronal', 'stomach', 'scoliosis', 'cystic', 'right parietal lobe', 'mid left subclavian vein', '7th rib', 'left aca and mca', 'cavum vergae', 'tumors, gallstones', 'small bowel', 'r hemidiaphragm', 'plain film', 'splenule', 'the posterior horn of the left lateral ventricle', 'chest', 'left temporal lobe', 'fat accumulations', 'ring-enhancing', 'cardiac region', 'pancreas', 'hyperintense', 'bilateral lungs', 'diverticulitis', 'abscess', 'right mca', 'smooth', '5cm', 'the pancreatic head', 'chest x-ray', 'the lungs', 'cancer', 'ascending colon', 'heterogeneous', 'left mca', 'micronodular', 'left lateral aspect of anterior peritoneum', 'ct', 'spleen', 'pneumothorax', 'mri - t2 weighted', 'respiratory \\tcardia c\\tmusculoskeletal', 'thalami, left occipital lobe, brainstem and left cerebellum', 'pleural plaques', 'regression of left frontal mass', 'aorta', 'ascites', 'kidneys', 'ureteral obstruction', 'cardiomegaly with pulmonary edema', 'sella and suprasellar cistern', 'basal ganglia (caudate and putamen)', 'superior', 'right superior cavoatrial junction', 'right of the midline, superior to the right hilum', 'cardiopulmonary', 'heart \\tlungs', 'csf is white', 'bilateral cerebellum', 'the right frontal lobe', 'motor weakness, sensory deficits, and left neglect', 'axial', 'left mid lung', 'middle mogul', 'left thalamus and basal ganglia', 'smaller', 'air?', 'pineal region', 'l2', 'basilar artery', 'pneumonia', 'congenital developmental disorder? history of surgery and past manipulation?', 'parietal and occipital lobes', 'non-contrast ct', 'the pancreas', 'right sided pleural effusion', 'infarcts', 'outside', 'width of aorta', 'csf is white.', 'right lenticular nucleus', '4th and 5th', 'pulmonary/lymphatic', 'no', 'head/neck ct', 'fatty infiltration', 'if the heart diameter is greater than half the diameter of the thoracic cavity.', 'the diaphragm', 'frontal and occipital', 'infiltrative', 'the extremities', 'right convexity', 'hypodense', 'l2-3', 't2 weighted mri', 'well-circumscribed', 'left upper lobe', 'right cerebellum', 'the base of the cecum', 'left hemisphere', 'acute stroke', 'elliptical', 'volume loss', '10-20 minutes', 'jejunum', 'genetic', 'stones, cancer, infection, anatomic variants', 'aorta is bright', 'basilar artery thrombosis', 'caudate, putamen, left parietal', 'right lobe', 'right vs left sided pathology', 'moderate edema', 'in the bowel', 'gastrointestinal', 'in the right hilum', 'pituitary fossa', 'solid', 'hemorrhage', 'nodules', 'right sylvian fissure', 'basal ganglia', '5 cm', 'the liver', 'double arch', 'gi', 'gallstones', 'chest radiograph', 'less enhancement', 'infarcted areas', 'nephroblastomatosis', 'haustra', 'xray', 'pericholecystic fluid', 'partial silhouetting', 'crescent', 'paratracheal area', 'in the thorasic aorta', 'sacroiliac joint', 'mr-flair', 'loculated', 'bilateral pleural effusion', 'plain film x-ray', 'short section irregular contour', 'base', 'toxoplasma, lymphoma, abscesses, other brain tumors', 'chest xray', 'mucosal hyperemia', 'pleural effusion', 'central hyperintensity and surrounding hypointensity', 'suprasellar cistern', 'chronic sinusitis vs. hemorrhage', 'embolus', 'abdominal pain', 'lateral ventricles', 'cva', 'above the clavicles bilaterally', 'right side of the trachea', 'ultrasound', 'cystic duct is more tortuous', 'mri/flair', 'necrotic tissue', 'posteroanterior', 'lentiform', \" superficial to the patient's skin\", 'xray - plain film', 'bowel contents light up on image', 'extraluminal air and small fluid collection', 'blind loop syndrome', 'trace the gallbladder emptying?', 'exophytic cyst', 'nodular opacities', 'posterior to the gastric antrum', 'blood vessel', 'retroperitoneum; retroperitoneal space', 'decreased muscle bulk', 'nothing', 'ring enhancing lesion in the left occipital lobe', 'cerebellum', 'fat', 'diverticuli', 'prior surgery', 'portal vein', 'csf is brightly lit', 'posterior brain', 'hydropneumothorax', 'choroid plexus', 'abnormal', 'oral and iv', 'adjacent to the appendix', 'all three vascular distributions', 'gadolinium', 'kidney cyst', 'appendix', 'brain', 'imaging artifacts', 'medical process', 'varicocele', 'mri-dwi', 'descending colon', 'fluid in the pleural space', '2.5cm x 1.7cm x 1.6cm', 'gallbladder', 'adjacent to vertebrae', 'right colon', 'gray matter', 'white matter plaques', 'lungs', 'mri', 'ruq pain, jaundice,weight loss?', 'axial plane', '5.6cm focal, predominantly hypodense', 'medial and lateral rectus', 'the left costophrenic angle is blunted', 'hyperintensity of the left basal ganglia', 'pa xray', 'right vertebral artery sign', 'right', 'bilateral frontal lobes and body of corpus callosum', 'metastasis', 'chest x ray', 'location of the contrast?', 'right lung base', 'proximal aspect of the appendix', 'left apical pneumothorax', 'retrocardiac', '3.4 cm', 'right lower lobe', 'temporal lobe', 'isointense', 'bilateral', 'aorta enhancement', 'lower lung fields', 'right frontal lobe', 'almost entire right side', 'epidural hematoma', 'large bowel', 'stomach bubble', 't2-mri', 'temporal and lateral occipital lobes', 'air', 'right subdural hematoma', '5mm', 'right hemisphere', 'right lung', 'x-ray', 'dwi diffusion weighted', 'posterior fossa', 'cerebrum and lateral ventricles', 'normal', 'underneath the right hemidiaphragm', 'the small intestines', 'lying supine with their feet towards the screen', 'hypointense', 'high on the image', 'parasitic', 'multiple sclerosis', 'with contrast', 'man', 'contrast', 'necrosis', 'bleeding in the right posteroinferior cerebellum', 'punctate', 'hypodense lesion', 'right parietal', 'ap', 'on the right shoulder.', 'cardiomegaly', 'intestines', 'hepatocellular carcioma', 'adenopathy', 'right lower lateral lung field', 'cystic lesions', 'the aorta and the inferior vena cava', 'subarachnoid', 'medial rectus', 'mri diffusion weighted', 'loss of normal gray-white matter junction', 'mid abdomen', 'more acute means more inflammation-leading to enhancement?', 'not seen here', 'maxillary sinuses', 'enlarged, fluid-filled', 'left occipital lobe', 'abdomen and pelvis', 'lung', 'hyperinflation', 'adipose tissue', 'in the bowels', 'diffuse', 'head of the pancreas', 'respiratory system', '3rd rib', 't2 weighted', 'mr flair', 'yes', 'psoas muscles', 'surrounding tissue', 'below the 7th rib in the right lung.', 'rounded, well-defined pulmonary nodules varying in size and pattern', 'right lung hilum', 'pres', 'left thalamus', 'reduced sulci', 'the left occipital lobe', 'it is shifted to the right', 'radiolucent', 'exterior', 'blind-ending loop of bowel arising from the cecum', 'pons', 'left hepatic lobe', 'omental caking', 'right cerebellopontine angle', 'vasculature', 'suprasellar', 'one', 'anterior to the transverse colon', 'air fluid level', 'left cerebellum', 'diaphragm', 'intestine', 'ring enhancing lesion in the right frontal lobe', 'right sided aortic arch', 'c-t ratio', 'ct with contrast', 'black', 'occipital lobe', 'a catheter', 'bronchiectasis', 'female', 'extra-axial and at the right choroidal fissure', 'white matter', 'the left kidney', 'iv contrast', 'horsehoe kidney', 'free air', 'white versus grey matter brightness', 'quadrantopia, aphasia, memory deficit, etc.', 'on top of the patient', 'supine (see air-fluid level)', 'basal ganglia, cerebellum, cerebral cortex', 'abnormal hyperintensity in the right occipital lobe', 'fluid', '~15 minutes\\tpotentially faster with newer imaging systems', 'the right mainstem bronchus is more in line with the trachea than the left', 'left parietal lobe', 'the aorta', 'peritoneum', 'asymmetric', 'biopsy', 'soft tissue mass in the region of the terminal ileum with mesenteric lymphadenopathy', 'it is less than half the width of the thorax', 'nucleus pulposus', 'non-enhanced', 'emphysema', 'xr', 'cirrhosis', 'anterior surface', 'bilateral parietal lobes', 'ms plaques', 'right lateral ventricle', 'none', 'supratentorial', 'portal vein occlusion', 'more dense', 'csf', 'a bit', 'imagine the patient is laying down and you are looking from the feet', 'both', 'nipple location', 'pacemaker', 'viral', 'infection', 'breasts', 'right lobe of the liver', 'midline', 'appendicitis', 'lungs \\tbony thoracic cavit y\\tmediastinum and great vessels', 'posterior lung seen in the image section', 'in the midline', 'cns', 'distal basilar artery', 'the brain', '6.5 x 6.2 x 8.8cm', 'on the left', 'edematous', 'costophrenic angle blunting', 'psoas major muscle', 'small subdural hematoma with cerebral edema', 'calcification', 'ring-enhancing lesion', 'in the vasculature', 'it is enlarged with prominence of the aortic knob', 'upper left', 'right temporal lobe', 'calcifications', 'skull \\tcartilage and medulla', 'early hemorrhage', 'thickening of bronchi', 'mri - t1 weighted', 'shrunken and nodular', 'right pca', 'the abdomen', 'contrast ct with gi and iv contrast', 'psoas muscle', 'calcified atherosclerosis', 'anterior mediastinum', 'blunting of the costophrenic angle, loss of the right hemidiaphragm and right heart border', 'cartilage is not well viewed by x rays', 'pulmonary nodules', 'single lung nodule', 'mixed intensity', 'right posteroinferior cerebellum', 'oculomotor nerve (cn iii) and trigeminal nerves (cn v)', 'plicae circulares', 'r frontal lobe', 'ischemia', 'fat stranding around the appendix, thickened appendiceal walls, dilated appendix, and an appendicolith is seen as well', 'above', 'sigmoid flexture of the colon', 'lung markings present all the way laterally to the ribs', 'not sure', 'right paratracheal mass lesion', 'in the cortex and basal ganglia bilaterally', 'hip bones', 'lateral film as well as pa', 'mass', 'less dense', '4th ventricle', 'less than half the thorax', 'the right bronchus', 'right subclavian vein', 'the 3rd ventricle and the lateral ventricles', 'mediport', 'left lobe mass 1.5 x 1.8 cm', 'atherosclerotic calcification', 'infarct', 'both sides', 'flair mri', 'left side', 'periappendiceal fluid and fat stranding', 'upper right lobe', 'biconvex', 'sternal wires', 'vascular', 'anterior cerebrum', 'mr - adc map', 'lateral and third ventricular hydrocephalus', \"on the patient's left\", 'coronal plane?', 'posterior-anterior', 'cortical ribbon of right occipital lobe with extension into right posterior temporal lobe', 'a bullous lesion', 'pancreatic body', 'posterior to the appendix', 'right side', 'dwi', 'left', 'viral/inflammatory', 'mri-flair', 'left kidney', 'cardiovascular']\n",
            "1669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02w3BwVSmfIQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "343f3fdfe6f54390a75b1ec40c6d77f6",
            "8ddba6008295449cb43850dd5eb034ed",
            "a65941c9ec8d40359079b865c75819d6",
            "f6a15c2c421e4c47a98cd86d3b69a6da",
            "876ff3bc08924349a5aa90e43bf971d0",
            "f41722472fee46d599387d19f968838a",
            "9daf595d34a844bdbb81aefd9ab5e157",
            "b702f8f65cb24abea18a99d3915457b0"
          ]
        },
        "outputId": "64b98a2d-423e-4f38-a992-d20c64a085be"
      },
      "source": [
        "class ImageFolderWithPaths(datasets.ImageFolder):\n",
        "    \"\"\"Custom dataset that includes image file paths. Extends\n",
        "    torchvision.datasets.ImageFolder\n",
        "    \"\"\"\n",
        "\n",
        "    # override the __getitem__ method. this is the method that dataloader calls\n",
        "    def __getitem__(self, index):\n",
        "        # this is what ImageFolder normally returns \n",
        "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
        "        # the image file path\n",
        "        path = self.imgs[index][0].split('/')[-1]\n",
        "        # make a new tuple that includes original and the path\n",
        "        tuple_with_path = (original_tuple + (path,))\n",
        "        return tuple_with_path\n",
        "\n",
        "\n",
        "img_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),                                     \n",
        "])\n",
        "\n",
        "\n",
        "imgdir = 'drive/My Drive/Colab Notebooks/MVQA/VQA-RAD'\n",
        "imgdataset = ImageFolderWithPaths(imgdir, transform=img_transforms)\n",
        "imgdataloader = torch.utils.data.DataLoader(imgdataset, batch_size=1, shuffle=False)\n",
        "\n",
        "vgg2 = VGG16.VGG16(num_classes=10)\n",
        "\n",
        "#comment this line to cancel pre-training\n",
        "vgg2.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/vggmodel.pt'))\n",
        "\n",
        "vgg2.cuda()\n",
        "vgg2.eval()\n",
        "\n",
        "dic = {}\n",
        "\n",
        "for batch in imgdataloader:\n",
        "    \n",
        "    img,label,path = batch\n",
        "    img = Variable(img).cuda()\n",
        "    \n",
        "    count = 0\n",
        "    for layer in vgg2.features.features:\n",
        "        img = layer(img)\n",
        "        count = count+1\n",
        "        if count == 30:\n",
        "            break\n",
        "    \n",
        "    img = img.view(-1, 512, 196).transpose(1, 2)\n",
        "    img_feature_numpy = img.detach().cpu().numpy() \n",
        "    dic[path[0].split('.')[0]] = img_feature_numpy\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "343f3fdfe6f54390a75b1ec40c6d77f6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=553433881), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyOi3U3HrNtT"
      },
      "source": [
        "que_model = QuestionEmbedding(len(dicQuestion),500,1024,1024,2,0.4,2248,1)\n",
        "img_model = ImageEmbedding(hidden_size=1024)\n",
        "attention_model = Attention(1024,512,196,517,0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy6Z2kwvrN7C",
        "outputId": "7647cca4-60b0-449e-ccbb-9a4eb1b70dc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    que_model.cuda()\n",
        "    img_model.cuda()\n",
        "    attention_model.cuda()\n",
        "    print('gpu used!')\n",
        "    \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_parameter_group = [\n",
        "                             {'params': que_model.parameters()},\n",
        "                             {'params': img_model.parameters()},\n",
        "                             {'params': attention_model.parameters()}\n",
        "                            ]\n",
        "\n",
        "#optimizer = torch.optim.SGD(optimizer_parameter_group,\n",
        "#                                lr=0.0004,\n",
        "#                                momentum=0.9)\n",
        "\n",
        "optimizer = torch.optim.Adam( optimizer_parameter_group,\n",
        "    \t\t                      eps=1e-8,\n",
        "\t\t                          lr=0.0004)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpu used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW28Fbz4LLgj",
        "outputId": "8b710ee7-df5d-403a-dd56-f5eee0cdbab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 20\n",
        "\n",
        "#training\n",
        "all_loss_store = []\n",
        "loss_store = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('epoch: ',epoch)\n",
        "    running_loss = 0.0\n",
        "    right_answer = 0\n",
        "    for i in range(len(ques_one_hot_train)):\n",
        "      question = torch.tensor(np.array(ques_one_hot_train[i], dtype=np.float32))\n",
        "      ans = torch.tensor(np.array([answer_to_vec_train[i]],dtype=np.long))\n",
        "      image = torch.tensor(dic[imgs_name_train[i]])\n",
        "      if torch.cuda.is_available():\n",
        "          image = image.cuda()\n",
        "          question = question.cuda()\n",
        "          ans = ans.cuda()\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          img_emb = img_model(image)\n",
        "          ques_emb  =que_model(question)\n",
        "          #print(img_emb.shape)\n",
        "          #print(ques_emb.shape)\n",
        "          output = attention_model(ques_emb, img_emb)\n",
        "          \n",
        "          pred = torch.max(output, 1)[1]\n",
        "          if pred == ans:\n",
        "             right_answer = right_answer + 1\n",
        "\n",
        "          loss = criterion(output,ans)\n",
        "\n",
        "          all_loss_store.append(loss.item())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "    loss_store.append(running_loss)\n",
        "    print('training loss:',(running_loss/len(ques_one_hot_train)))\n",
        "    print('training accuracy:',(right_answer/len(ques_one_hot_train)))\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/MVQA/san.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  p1 = self.sf(h1_emb.view(-1, self.img_seq_size)).view(B, 1, self.img_seq_size)\n",
            "/content/drive/My Drive/Colab Notebooks/MVQA/san.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  p2 = self.sf(h2_emb.view(-1, self.img_seq_size)).view(B, 1, self.img_seq_size)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 3.088079213045596\n",
            "training accuracy: 0.38609660574412535\n",
            "epoch:  1\n",
            "training loss: 1.8502498159364995\n",
            "training accuracy: 0.529210182767624\n",
            "epoch:  2\n",
            "training loss: 1.1908419923589373\n",
            "training accuracy: 0.679177545691906\n",
            "epoch:  3\n",
            "training loss: 0.7145925038165897\n",
            "training accuracy: 0.8018929503916449\n",
            "epoch:  4\n",
            "training loss: 0.44828213817456997\n",
            "training accuracy: 0.8723890339425587\n",
            "epoch:  5\n",
            "training loss: 0.3359221456178486\n",
            "training accuracy: 0.9060052219321149\n",
            "epoch:  6\n",
            "training loss: 0.23845329176508104\n",
            "training accuracy: 0.9308093994778068\n",
            "epoch:  7\n",
            "training loss: 0.21381870069718548\n",
            "training accuracy: 0.9378263707571801\n",
            "epoch:  8\n",
            "training loss: 0.18283734935537643\n",
            "training accuracy: 0.9454960835509139\n",
            "epoch:  9\n",
            "training loss: 0.16303426151798536\n",
            "training accuracy: 0.9525130548302873\n",
            "epoch:  10\n",
            "training loss: 0.1507927653800414\n",
            "training accuracy: 0.9536553524804178\n",
            "epoch:  11\n",
            "training loss: 0.14960554973574905\n",
            "training accuracy: 0.9559399477806788\n",
            "epoch:  12\n",
            "training loss: 0.14878005800919494\n",
            "training accuracy: 0.9575718015665796\n",
            "epoch:  13\n",
            "training loss: 0.1378945400525011\n",
            "training accuracy: 0.9616514360313316\n",
            "epoch:  14\n",
            "training loss: 0.13772261269176597\n",
            "training accuracy: 0.9631201044386423\n",
            "epoch:  15\n",
            "training loss: 0.13519951222615204\n",
            "training accuracy: 0.9631201044386423\n",
            "epoch:  16\n",
            "training loss: 0.11910303431628734\n",
            "training accuracy: 0.9657310704960835\n",
            "epoch:  17\n",
            "training loss: 0.12010729771992557\n",
            "training accuracy: 0.9686684073107049\n",
            "epoch:  18\n",
            "training loss: 0.10689432893736245\n",
            "training accuracy: 0.9727480417754569\n",
            "epoch:  19\n",
            "training loss: 0.11451637017353397\n",
            "training accuracy: 0.9689947780678851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itwWRoXa0htA",
        "outputId": "965a4ad6-279e-4b60-e8bf-332052cf4b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "que_model.eval()\n",
        "img_model.eval()\n",
        "attention_model.eval()\n",
        "\n",
        "print(len(questions_test))\n",
        "print('test!')\n",
        "if torch.cuda.is_available():\n",
        "    right_answer_test = 0\n",
        "    right = []\n",
        "    for i in range(len(questions_test)):\n",
        "      question = torch.tensor(np.array(ques_one_hot_test[i], dtype=np.float32))\n",
        "      ans = torch.tensor(np.array([answer_to_vec_test[i]],dtype=np.long))\n",
        "      image = torch.tensor(dic[imgs_name_test[i]])\n",
        "      if torch.cuda.is_available():\n",
        "          image = image.cuda()\n",
        "          question = question.cuda()\n",
        "          ans = ans.cuda()\n",
        "\n",
        "          img_emb = img_model(image)\n",
        "          ques_emb  =que_model(question)\n",
        "\n",
        "          output = attention_model(ques_emb, img_emb)\n",
        "          \n",
        "          pred = torch.max(output, 1)[1]\n",
        "          if pred == ans:\n",
        "             right_answer_test = right_answer_test + 1\n",
        "             right.append(1)\n",
        "          else:\n",
        "             right.append(0)\n",
        "\n",
        "    print('test accuracy:',(right_answer_test/len(questions_test)))\n",
        "    print(right)\n",
        "    print(right_answer_test)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "451\n",
            "test!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/MVQA/san.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  p1 = self.sf(h1_emb.view(-1, self.img_seq_size)).view(B, 1, self.img_seq_size)\n",
            "/content/drive/My Drive/Colab Notebooks/MVQA/san.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  p2 = self.sf(h2_emb.view(-1, self.img_seq_size)).view(B, 1, self.img_seq_size)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy: 0.5609756097560976\n",
            "[0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1]\n",
            "253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vd9o2plmSwL"
      },
      "source": [
        "#import os\n",
        "#model_dir = '/content/drive/My Drive/Colab Notebooks/'\n",
        "#torch.save(que_model.state_dict(), os.path.join(model_dir, 'question_model.pkl'))\n",
        "#torch.save(img_model.state_dict(), os.path.join(model_dir, 'image_model.pkl'))\n",
        "#torch.save(attention_model.state_dict(), os.path.join(model_dir, 'attention_model.pkl'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}